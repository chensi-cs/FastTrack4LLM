#!/usr/bin/env python3
# -*- coding: utf-8 -*-  # å£°æ˜ç¼–ç ä¸º UTF-8
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
from models.llama_model import Llama1ForCausalLM , Llama3ForCausalLM
from utils.config import ChatConfig
from models.add_lora import apply_lora, save_lora, load_lora

def init_model(config,ischeckpoint=True):
    # model = Llama1ForCausalLM(config)
    model = Llama3ForCausalLM(config)

    
    # strict=Trueï¼ˆé»˜è®¤ï¼‰ï¼šä¸¥æ ¼æ£€æŸ¥æƒé‡æ–‡ä»¶ä¸­çš„å‚æ•°åä¸æ¨¡å‹ç»“æ„ä¸­çš„å‚æ•°åæ˜¯å¦å®Œå…¨åŒ¹é…ã€‚è‹¥æœ‰ä¸åŒ¹é…ï¼ˆå¦‚ç¼ºå°‘å‚æ•°æˆ–å¤šäº†æ— å…³å‚æ•°ï¼‰ï¼Œä¼šæŠ›å‡º RuntimeErrorã€‚
    
    if ischeckpoint:
        # åŠ è½½æ¨¡å‹æƒé‡æ–‡ä»¶
        print(f"åŠ è½½checkpointæ¨¡å‹æƒé‡æ–‡ä»¶: {config.model_path}")
        checkpoint = torch.load(config.model_path,map_location = config.device)
        state_dict = checkpoint['model_state_dict']
        model.load(state_dict, strcit=True)
    else:
        print(f"åŠ è½½æœ€ç»ˆæ¨¡å‹æƒé‡æ–‡ä»¶: {config.model_path}")
        model.load_state_dict(torch.load(config.model_path, map_location=config.device), strict=True)

    if config.model_type == 'lora':
        apply_lora(model, rank=config.lora_rank, device=config.device)
        load_lora(model, config.lora_path)  # åŠ è½½ LoRA æƒé‡

    print(f"æ¨¡å‹: {model}")

    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path)

    # è®¡ç®—æ¨¡å‹æ€»å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:.3f} M")  # ä¿ç•™ä¸¤ä½å°æ•°

    # è®¡ç®—å¯è®­ç»ƒå‚æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    print(f'LLMå¯è®­ç»ƒå‚æ•°é‡: {trainable_params:.3f} M')

    return model, tokenizer

def prompt_template():
    prompts = [
        'å¤§æ¨¡å‹åŸºæœ¬åŸç†ä»‹ç»',
        'æ¹–å—ç¾é£Ÿæœ‰ä»€ä¹ˆ',
        'ä¸­å›½æœ‰å“ªäº›æ—…æ¸¸æ™¯ç‚¹',
        'åŒ—äº¬æœ‰ä»€ä¹ˆç‰¹è‰²ç¾é£Ÿ',
        'æ·±åœ³ä½äºä¸­å›½çš„å“ªé‡Œ',
        'åŒ—äº¬æœ‰ä»€ä¹ˆæ—…æ¸¸æ™¯ç‚¹'
    ]
    return prompts


def main():
    config = ChatConfig()
    config.model_path = 'all_logs/sft_20250819_172358_llama3/saved_models/llama3_sft.pt'  # æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
    # config.pretrain_path = 'saved_models/llama3_model.pt'  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    config.model_type = 'dpo'
    if config.model_type == 'lora':
        config.lora_path = 'all_logs/lora_20250818_212432_llama3/saved_models/llama3_lora.pt'
    
    config.flash_att = False
    config.use_moe = True  # æ˜¯å¦ä½¿ç”¨MoEæ¨¡å‹
    config.d_model = 768
    config.num_layers = 16
    config.hidden_dim = 3072
    model, tokenizer = init_model(config,ischeckpoint=False)
    model.eval()
    model.to(config.device)
    prompt_templates = prompt_template()

    # TextStreamer å®ç°æ–‡æœ¬çš„æµå¼è¾“å‡ºï¼Œå³ç”Ÿæˆçš„æ–‡æœ¬ä¼šåƒæ‰“å­—æœºä¸€æ ·é€ä¸ª token å®æ—¶æ˜¾ç¤ºï¼Œè€Œéç­‰å®Œæ•´ç»“æœç”Ÿæˆåä¸€æ¬¡æ€§è¾“å‡ºã€‚
    # skip_prompt=True,      # ä¸æ˜¾ç¤ºè¾“å…¥çš„æç¤ºæ–‡æœ¬
    # skip_special_tokens=True  # ä¸æ˜¾ç¤ºç‰¹æ®Štokenï¼ˆå¦‚<|EndOfText|>ï¼‰
    streamer = TextStreamer(tokenizer,skip_special_tokens=True,skip_prompt=True)
    messages = []

    test_mode = int(input(" è¯·è¾“å…¥æµ‹è¯•æ¨¡å¼ï¼š [0] è‡ªåŠ¨æµ‹è¯•  [1] æ‰‹åŠ¨æµ‹è¯•\n"))
    # iter(lambda: input('ğŸ‘¶: '), ''ï¼‰ç”¨äºåˆ›å»ºä¸€ä¸ªæŒç»­æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„è¿­ä»£å™¨ï¼Œç›´åˆ°ç”¨æˆ·è¾“å…¥ç©ºå­—ç¬¦ä¸²ï¼ˆç›´æ¥æŒ‰å›è½¦ï¼‰æ‰ä¼šåœæ­¢ã€‚
    for idx ,prompt in enumerate(prompt_templates if test_mode==0 else iter( lambda: input("ğŸ‘¶ï¼š"), "")):
        messages.append({"role":"user","content":prompt})
        print(f'ğŸ‘¶: {prompt}')
        new_prompt = tokenizer.bos_token + prompt

        input_ids =  tokenizer(
            str(new_prompt),
            truncation=True,
            return_tensors='pt'
        )
        # print(f"input_ids: {input_ids}")
        input_ids = input_ids.input_ids
        input_ids = input_ids.to(config.device)  

        # do_sample=True ?
        print('ğŸ¤–ï¸: ', end='')
        # outputs = model(input_ids=input_ids)  # å‰å‘ä¼ æ’­ï¼Œè·å–æ¨¡å‹è¾“å‡º
        # print("æ¨¡å‹è¾“å‡ºï¼š", outputs)
        output_ids = model.generate(
            input_ids=input_ids,                             # è¾“å…¥çš„ Token åºåˆ—ï¼ˆæ¨¡å‹ç”Ÿæˆçš„èµ·ç‚¹ï¼‰
            max_new_tokens=config.max_generate_len,  # æœ€å¤šç”Ÿæˆçš„æ–° Token æ•°é‡
            num_return_sequences=1,                  # ç”Ÿæˆçš„å€™é€‰åºåˆ—æ•°é‡
            do_sample=True,                          # å¯ç”¨é‡‡æ ·ç­–ç•¥ï¼ˆè€Œéè´ªå¿ƒæœç´¢ï¼‰
            streamer=streamer,                       # æµå¼è¾“å‡ºå™¨ï¼ˆå®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹ï¼‰
            temperature=0.85,                         # æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼‰
            top_p=0.85,                               # æ ¸é‡‡æ ·å‚æ•°ï¼ˆæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼‰
            pad_token_id=tokenizer.pad_token_id,     # å¡«å…… Token çš„ ID
            eos_token_id=tokenizer.eos_token_id,     # ç»“æŸ Token çš„ ID
            bos_token_id=tokenizer.bos_token_id     # å¼€å§‹ Token çš„ ID
        )

        response = tokenizer.decode(output_ids[0][input_ids.shape[1]:],skip_special_tokens=True)
        messages.append({"role":"assistant","content":response})
        print("----------------------------")

    print("å¯¹è¯å†å²ï¼š")
    for message in messages:
        print(message)
        
if __name__ == '__main__':
    main()
