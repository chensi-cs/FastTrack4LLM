#!/usr/bin/env python3
# -*- coding: utf-8 -*-  # å£°æ˜ç¼–ç ä¸º UTF-8
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
from models.llama_model import Llama1ForCausalLM 
from utils.config import ChatConfig

def init_model(config,ischeckpoint=True):
    model = Llama1ForCausalLM(config)
    # strict=Trueï¼ˆé»˜è®¤ï¼‰ï¼šä¸¥æ ¼æ£€æŸ¥æƒé‡æ–‡ä»¶ä¸­çš„å‚æ•°åä¸æ¨¡å‹ç»“æ„ä¸­çš„å‚æ•°åæ˜¯å¦å®Œå…¨åŒ¹é…ã€‚è‹¥æœ‰ä¸åŒ¹é…ï¼ˆå¦‚ç¼ºå°‘å‚æ•°æˆ–å¤šäº†æ— å…³å‚æ•°ï¼‰ï¼Œä¼šæŠ›å‡º RuntimeErrorã€‚
    # æ­£ç¡®å†™æ³•ï¼ˆæå–æ¨¡å‹æƒé‡ï¼‰
    if ischeckpoint:
        # åŠ è½½æ¨¡å‹æƒé‡æ–‡ä»¶
        print(f"åŠ è½½checkpointæ¨¡å‹æƒé‡æ–‡ä»¶: {config.model_path}")
        checkpoint = torch.load(config.model_path, map_location=config.device)
        state_dict = checkpoint['model_state_dict']  # åªå–æ¨¡å‹æƒé‡éƒ¨åˆ†

        # åŠ è½½åˆ°æ¨¡å‹
        model.load_state_dict(state_dict, strict=True)
    else:
        print(f"åŠ è½½æœ€ç»ˆæ¨¡å‹æƒé‡æ–‡ä»¶: {config.model_path}")
        model = load_state_dict(torch.load(config.model_path, map_location=config.device), strict=True)


    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path)

    # è®¡ç®—æ¨¡å‹æ€»å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:.3f} M")  # ä¿ç•™ä¸¤ä½å°æ•°

    # è®¡ç®—å¯è®­ç»ƒå‚æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    print(f'LLMå¯è®­ç»ƒå‚æ•°é‡: {trainable_params:.3f} M')

    return model, tokenizer

def prompt_template():
    prompts = [
        'å¤§æ¨¡å‹åŸºæœ¬åŸç†ä»‹ç»',
        'æ¹–å—ç¾é£Ÿæœ‰ä»€ä¹ˆ',
        'ä¸­å›½æœ‰å“ªäº›æ—…æ¸¸æ™¯ç‚¹',
    ]
    return prompts


def main():
    config = ChatConfig()
    config.model_path = 'checkpoints/checkpoint_epoch_1.pt'  # æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„

    model, tokenizer = init_model(config,ischeckpoint=True)
    model.eval()
    model.to(config.device)
    prompt_templates = prompt_template()

    # TextStreamer å®ç°æ–‡æœ¬çš„æµå¼è¾“å‡ºï¼Œå³ç”Ÿæˆçš„æ–‡æœ¬ä¼šåƒæ‰“å­—æœºä¸€æ ·é€ä¸ª token å®æ—¶æ˜¾ç¤ºï¼Œè€Œéç­‰å®Œæ•´ç»“æœç”Ÿæˆåä¸€æ¬¡æ€§è¾“å‡ºã€‚
    # skip_prompt=True,      # ä¸æ˜¾ç¤ºè¾“å…¥çš„æç¤ºæ–‡æœ¬
    # skip_special_tokens=True  # ä¸æ˜¾ç¤ºç‰¹æ®Štokenï¼ˆå¦‚<|EndOfText|>ï¼‰
    streamer = TextStreamer(tokenizer,skip_special_tokens=True )
    messages = []

    test_mode = int(input(" è¯·è¾“å…¥æµ‹è¯•æ¨¡å¼ï¼š [0] è‡ªåŠ¨æµ‹è¯•  [1] æ‰‹åŠ¨æµ‹è¯•\n"))
    # iter(lambda: input('ğŸ‘¶: '), ''ï¼‰ç”¨äºåˆ›å»ºä¸€ä¸ªæŒç»­æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„è¿­ä»£å™¨ï¼Œç›´åˆ°ç”¨æˆ·è¾“å…¥ç©ºå­—ç¬¦ä¸²ï¼ˆç›´æ¥æŒ‰å›è½¦ï¼‰æ‰ä¼šåœæ­¢ã€‚
    for idx ,prompt in enumerate(prompt_templates if test_mode==0 else iter( lambda: input("ğŸ‘¶ï¼š"), "")):
        messages.append({"role":"user","content":prompt})
        print(f'ğŸ‘¶: {prompt}')
        new_prompt = tokenizer.bos_token + prompt

        input_ids =  tokenizer(
            str(new_prompt),
            truncation=True,
            return_tensors='pt'
        )
        input_ids = input_ids.input_ids
        input_ids = input_ids.to(config.device)  

        # do_sample=True ?
        output_ids = model.generate(
            input_ids=input_ids,                             # è¾“å…¥çš„ Token åºåˆ—ï¼ˆæ¨¡å‹ç”Ÿæˆçš„èµ·ç‚¹ï¼‰
            max_new_tokens=config.max_generate_len,  # æœ€å¤šç”Ÿæˆçš„æ–° Token æ•°é‡
            num_return_sequences=1,                  # ç”Ÿæˆçš„å€™é€‰åºåˆ—æ•°é‡
            do_sample=True,                          # å¯ç”¨é‡‡æ ·ç­–ç•¥ï¼ˆè€Œéè´ªå¿ƒæœç´¢ï¼‰
            streamer=streamer,                       # æµå¼è¾“å‡ºå™¨ï¼ˆå®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹ï¼‰
            temperature=config.temperature,          # æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼‰
            top_p=config.top_p,                      # æ ¸é‡‡æ ·å‚æ•°ï¼ˆæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼‰
            pad_token_id=tokenizer.pad_token_id,     # å¡«å…… Token çš„ ID
            eos_token_id=tokenizer.eos_token_id,     # ç»“æŸ Token çš„ ID
            bos_token_id=tokenizer.bos_token_id     # å¼€å§‹ Token çš„ ID
            # attention_mask=input_ids.attention_mask  # æ³¨æ„åŠ›æ©ç ï¼ˆæ ‡è®°æœ‰æ•ˆ Tokenï¼‰
        )

        response = tokenizer.decode(output_ids[0][input_ids.shape[1]:],skip_special_tokens=True)
        messages.append({"role":"assistant","content":response})
        print(f'ğŸ¤–ï¸: {response}')
        print("----------------------------")

    print("å¯¹è¯å†å²ï¼š")
    for message in messages:
        print(message)
        
if __name__ == '__main__':
    main()
