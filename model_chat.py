import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
from models.llama_model import Llama1Model
from utils.config import ChatConfig

def init_model(config):
    model = Llama1Model(config)
    model.to(config.device)
    model.load_state_dict(torch.load(config.model_path,map_location=config.device),state=True)
    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path)

    # è®¡ç®—æ¨¡å‹æ€»å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:.3f} M")  # ä¿ç•™ä¸¤ä½å°æ•°

    # è®¡ç®—å¯è®­ç»ƒå‚æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    print(f'LLMå¯è®­ç»ƒå‚æ•°é‡: {trainable_params:.3f} M')

    return model, tokenizer

def prompt_template():
    prompts = [
        'å¤§æ¨¡å‹åŸºæœ¬åŸç†ä»‹ç»',
        'æ¹–å—ç¾é£Ÿæœ‰ä»€ä¹ˆ',
        'ä¸­å›½æœ‰å“ªäº›æ—…æ¸¸æ™¯ç‚¹',
    ]
    return prompts


def main():
    config = ChatConfig()
    config.model_path = config.model_path + 'llama1.pt'
    model, tokenizer = init_model(config)
    prompt_templates = prompt_template()

    # TextStreamer å®ç°æ–‡æœ¬çš„æµå¼è¾“å‡ºï¼Œå³ç”Ÿæˆçš„æ–‡æœ¬ä¼šåƒæ‰“å­—æœºä¸€æ ·é€ä¸ª token å®æ—¶æ˜¾ç¤ºï¼Œè€Œéç­‰å®Œæ•´ç»“æœç”Ÿæˆåä¸€æ¬¡æ€§è¾“å‡ºã€‚
    # skip_prompt=True,      # ä¸æ˜¾ç¤ºè¾“å…¥çš„æç¤ºæ–‡æœ¬
    # skip_special_tokens=True  # ä¸æ˜¾ç¤ºç‰¹æ®Štokenï¼ˆå¦‚<|EndOfText|>ï¼‰
    streamer = TextStreamer(tokenizer,skip_special_tokens=True )
    messages = []

    test_mode = int(input(" è¯·è¾“å…¥æµ‹è¯•æ¨¡å¼ï¼š\n [0] è‡ªåŠ¨æµ‹è¯• \n [1] æ‰‹åŠ¨æµ‹è¯•\n"))
    # iter(lambda: input('ğŸ‘¶: '), ''ï¼‰ç”¨äºåˆ›å»ºä¸€ä¸ªæŒç»­æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„è¿­ä»£å™¨ï¼Œç›´åˆ°ç”¨æˆ·è¾“å…¥ç©ºå­—ç¬¦ä¸²ï¼ˆç›´æ¥æŒ‰å›è½¦ï¼‰æ‰ä¼šåœæ­¢ã€‚
    for idx ,prompt in enumerate(prompt_templates if test_mode==0 else iter( lambda: input("ğŸ‘¶ï¼š"), "")):
        messages.append({"role":"user","content":prompt})
        print(f'ğŸ‘¶: {prompt}')
        new_prompt = tokenizer.bos_token + prompt

        input_ids =  tokenizer(
            str(new_prompt),
            truncation=True,
            return_tensors='pt'
        )
        input_ids = input_ids.input_ids.squeeze()
        input_ids.to(config.device)

        # do_sample=True ?
        output_ids = model.generate(
            input_ids=input_ids,                     # è¾“å…¥çš„ Token åºåˆ—ï¼ˆæ¨¡å‹ç”Ÿæˆçš„èµ·ç‚¹ï¼‰
            max_new_tokens=config.max_generate_len,  # æœ€å¤šç”Ÿæˆçš„æ–° Token æ•°é‡
            num_return_sequences=1,                  # ç”Ÿæˆçš„å€™é€‰åºåˆ—æ•°é‡
            do_sample=True,                          # å¯ç”¨é‡‡æ ·ç­–ç•¥ï¼ˆè€Œéè´ªå¿ƒæœç´¢ï¼‰
            streamer=streamer,                       # æµå¼è¾“å‡ºå™¨ï¼ˆå®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹ï¼‰
            temperature=config.temperature,          # æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼‰
            top_p=config.top_p,                      # æ ¸é‡‡æ ·å‚æ•°ï¼ˆæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼‰
            pad_token_id=tokenizer.pad_token_id,     # å¡«å…… Token çš„ ID
            eos_token_id=tokenizer.eos_token_id,     # ç»“æŸ Token çš„ ID
            attention_mask=input_ids.attention_mask  # æ³¨æ„åŠ›æ©ç ï¼ˆæ ‡è®°æœ‰æ•ˆ Tokenï¼‰
        )

        response = tokenizer.decode(output_ids[0][input_ids.shape[1]:],skip_special_tokens=True)
        messages.append({"role":"assistant","content":response})
        print(f'ğŸ¤–ï¸: {response}')
        print("----------------------------")

    print("å¯¹è¯å†å²ï¼š")
    for message in messages:
        print(message)
        
if __name__ == '__main__':
    main()
